{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70361c7",
   "metadata": {},
   "source": [
    "### CIS 9: Lab 4\n",
    "Natural Language Processing: Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "b1fef7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Name: Arnav Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95b350",
   "metadata": {},
   "source": [
    "In this lab you will train an ML model to categorize news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "984e42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196de76b",
   "metadata": {},
   "source": [
    "The [BBC News](https://www.bbc.com/news) is a British news organization that reports on current events around the world. In this exercise you will train an NLP model to categorize the topics of news articles. The model will determine whether a news articles is on sports, politics, etc.\n",
    "\n",
    "The training data are from BBC News and have been preprocessed for ML. The training input file is `news.csv` ([source](https://www.kaggle.com/datasets/dheemanthbhat/bbc-full-text-preprocessed?select=docs_stage_3_preprocessed.csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63724b6",
   "metadata": {},
   "source": [
    "1. __Read data from _news.csv_ into a DataFrame__.<br>\n",
    "Then __print the number of rows and columns of the DataFrame__<br>\n",
    "and __print the first 5 rows__ to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "63381186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocId</th>\n",
       "      <th>DocTextlen</th>\n",
       "      <th>DocText</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>...</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>DocType</th>\n",
       "      <th>FileSize</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>DocCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_001</td>\n",
       "      <td>2553</td>\n",
       "      <td>ad sale boost time_warner profit quarterly pro...</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>2560</td>\n",
       "      <td>../input/bbc-full-text-document-classification...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_002</td>\n",
       "      <td>2248</td>\n",
       "      <td>dollar gain greenspan speech dollar hit high l...</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>2252</td>\n",
       "      <td>../input/bbc-full-text-document-classification...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_003</td>\n",
       "      <td>1547</td>\n",
       "      <td>yukos unit buyer face loan claim owner embattl...</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>1552</td>\n",
       "      <td>../input/bbc-full-text-document-classification...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_004</td>\n",
       "      <td>2395</td>\n",
       "      <td>high fuel price hit ba profit british_airways ...</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>2412</td>\n",
       "      <td>../input/bbc-full-text-document-classification...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_005</td>\n",
       "      <td>1565</td>\n",
       "      <td>pernod takeover talk lift domecq share uk drin...</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>1570</td>\n",
       "      <td>../input/bbc-full-text-document-classification...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DocId  DocTextlen                                            DocText  ADJ  \\\n",
       "0  B_001        2553  ad sale boost time_warner profit quarterly pro...   31   \n",
       "1  B_002        2248  dollar gain greenspan speech dollar hit high l...   33   \n",
       "2  B_003        1547  yukos unit buyer face loan claim owner embattl...   11   \n",
       "3  B_004        2395  high fuel price hit ba profit british_airways ...   36   \n",
       "4  B_005        1565  pernod takeover talk lift domecq share uk drin...   15   \n",
       "\n",
       "   ADP   ADV   AUX  CCONJ  DET  NOUN  ...  PUNCT  SCONJ   SYM  VERB    X  \\\n",
       "0   61  15.0  15.0   13.0   28   114  ...     55    3.0   9.0    53  0.0   \n",
       "1   54  15.0  21.0    9.0   44    99  ...     43    5.0   2.0    43  0.0   \n",
       "2   32   3.0  15.0    4.0   25    71  ...     26    3.0   4.0    42  0.0   \n",
       "3   53  16.0  17.0    8.0   26   114  ...     62    8.0  10.0    45  0.0   \n",
       "4   32   5.0  13.0    8.0   14    68  ...     35    5.0   3.0    26  0.0   \n",
       "\n",
       "   INTJ   DocType  FileSize  \\\n",
       "0   0.0  Business      2560   \n",
       "1   0.0  Business      2252   \n",
       "2   0.0  Business      1552   \n",
       "3   0.0  Business      2412   \n",
       "4   0.0  Business      1570   \n",
       "\n",
       "                                            FilePath  DocCat  \n",
       "0  ../input/bbc-full-text-document-classification...       0  \n",
       "1  ../input/bbc-full-text-document-classification...       0  \n",
       "2  ../input/bbc-full-text-document-classification...       0  \n",
       "3  ../input/bbc-full-text-document-classification...       0  \n",
       "4  ../input/bbc-full-text-document-classification...       0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"news.csv\")\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc866d13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64c9fd",
   "metadata": {},
   "source": [
    "2. Data cleaning\n",
    "\n",
    "2a. __Print all the column labels__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "d70f41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DocId' 'DocTextlen' 'DocText' 'ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET'\n",
      " 'NOUN' 'NUM' 'PART' 'PRON' 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X'\n",
      " 'INTJ' 'DocType' 'FileSize' 'FilePath' 'DocCat']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3e0a6",
   "metadata": {},
   "source": [
    "2b. Since the data have been preprocessed, each row or news article has multiple features, some of which we don't need for our ML training purpose.\n",
    "\n",
    "The column labels that are all uppercase such as ADJ, ADV, NOUN... denote the count of adjectives, adverbs, nouns... that are in the article. We can remove these columns because Parts of Speech are not used by the MultinomialMB model.\n",
    "\n",
    "The columns we want to keep are:\n",
    "- DocText: contains the news articles\n",
    "- DocType: categories of the news articles, as strings\n",
    "- DocCat: categories of the news articles, as numbers\n",
    "\n",
    "Given that the columns containing Parts of Speech can be removed due to the reason above, create a Raw NBConvert cell to __explain why the other columns can also be removed__, so that we only keep the 3 columns DocText, DocType, and DocCat."
   ]
  },
  {
   "cell_type": "raw",
   "id": "00697f39",
   "metadata": {},
   "source": [
    "Filesize/path is irrelevant to the model, and DOCID can be replaced by index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ca1f0",
   "metadata": {},
   "source": [
    "2c. __Create a DataFrame with the 3 columns__ that you want to keep.<br>\n",
    "Then __print the first 5 rows__ of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "2cca8808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocText</th>\n",
       "      <th>DocType</th>\n",
       "      <th>DocCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad sale boost time_warner profit quarterly pro...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollar gain greenspan speech dollar hit high l...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yukos unit buyer face loan claim owner embattl...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high fuel price hit ba profit british_airways ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pernod takeover talk lift domecq share uk drin...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             DocText   DocType  DocCat\n",
       "0  ad sale boost time_warner profit quarterly pro...  Business       0\n",
       "1  dollar gain greenspan speech dollar hit high l...  Business       0\n",
       "2  yukos unit buyer face loan claim owner embattl...  Business       0\n",
       "3  high fuel price hit ba profit british_airways ...  Business       0\n",
       "4  pernod takeover talk lift domecq share uk drin...  Business       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.drop(columns=['DocId', 'DocTextlen', 'ADJ','ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'INTJ', 'FileSize', 'FilePath'], inplace=True),'\\n'\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f5c58",
   "metadata": {},
   "source": [
    "2d. __Shorten the column labels__ by removing the 'Doc' from each label and lowercase all letters.<br>\n",
    "Then __print the first 5 rows__ of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "43c42752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad sale boost time_warner profit quarterly pro...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollar gain greenspan speech dollar hit high l...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yukos unit buyer face loan claim owner embattl...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high fuel price hit ba profit british_airways ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pernod takeover talk lift domecq share uk drin...</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      type  cat\n",
       "0  ad sale boost time_warner profit quarterly pro...  Business    0\n",
       "1  dollar gain greenspan speech dollar hit high l...  Business    0\n",
       "2  yukos unit buyer face loan claim owner embattl...  Business    0\n",
       "3  high fuel price hit ba profit british_airways ...  Business    0\n",
       "4  pernod takeover talk lift domecq share uk drin...  Business    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.rename(columns={\"DocText\":\"text\", \"DocType\":\"type\", \"DocCat\":\"cat\"})\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c5ce6",
   "metadata": {},
   "source": [
    "2e. __Check and remove any NaN__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "0b61ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706644ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show result of checking NaN             -1/2pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea79dfd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a79cd",
   "metadata": {},
   "source": [
    "3. Analyze data\n",
    "\n",
    "3b. __Show the count of each DocType categories__<br>\n",
    "and then __show the count of each DocCat categories__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "f744e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "Business         510\n",
      "Entertainment    381\n",
      "Politics         413\n",
      "Sport            506\n",
      "Tech             395\n",
      "dtype: int64\n",
      "cat\n",
      "0    510\n",
      "1    381\n",
      "2    413\n",
      "3    506\n",
      "4    395\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['type']).size())\n",
    "print(df.groupby(['cat']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575542a",
   "metadata": {},
   "source": [
    "3b. The output seems to show that the there is a one-to-one correspondence between the strings in DocType and numbers in DocCat.\n",
    "\n",
    "Write code to __print the proof that they correspond with each other__. This means to show that all \"Business\" DoctType are 0 in DocCat, all \"Sport\" DocType are 3 in DocCat, etc.\n",
    "\n",
    "_Challenge: write a loop to check and print the 5 results, instead of copy-paste code 5 times_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "9a938117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(((df['type'] == 'Business') & (df['cat'] == 0)).any())\n",
    "print(((df['type'] == 'Entertainment') & (df['cat'] == 1)).any())\n",
    "print(((df['type'] == 'Politics') & (df['cat'] == 2)).any())\n",
    "print(((df['type'] == 'Sport') & (df['cat'] == 3)).any())\n",
    "print(((df['type'] == 'Tech') & (df['cat'] == 4)).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8647499",
   "metadata": {},
   "source": [
    "3c. __Create a lookup table__ which is a dictionary where each unique DocCat value is the key, and the corresponding DocType string is the value.<br>\n",
    "Then __print the lookup table__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "576e94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Business', 1: 'Entertainment', 2: 'Politics', 3: 'Sport', 4: 'Tech'}\n"
     ]
    }
   ],
   "source": [
    "d = dict(zip(df['cat'],df[\"type\"]))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e118f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbab3f",
   "metadata": {},
   "source": [
    "4. Preparing data for ML\n",
    "\n",
    "4a. Now that you've proven that DocType and DocCat have the same data, choose the column that makes it less work for you to use the ML model, then __remove one of the columns__. <br>\n",
    "Then __show the first 5 rows__ of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "e76a884d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad sale boost time_warner profit quarterly pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollar gain greenspan speech dollar hit high l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yukos unit buyer face loan claim owner embattl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high fuel price hit ba profit british_airways ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pernod takeover talk lift domecq share uk drin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cat\n",
       "0  ad sale boost time_warner profit quarterly pro...    0\n",
       "1  dollar gain greenspan speech dollar hit high l...    0\n",
       "2  yukos unit buyer face loan claim owner embattl...    0\n",
       "3  high fuel price hit ba profit british_airways ...    0\n",
       "4  pernod takeover talk lift domecq share uk drin...    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.drop(columns=['type'], inplace=True),'\\n'\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d628d12",
   "metadata": {},
   "source": [
    "4b. __Create the X and y datasets__ and __print the shape__ of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "5649e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2205,)\n",
      "(2205,)\n"
     ]
    }
   ],
   "source": [
    "X=df.text\n",
    "y=df.cat\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c84907",
   "metadata": {},
   "source": [
    "4c. Since the training data are already preprocessed. We want to take a look at one sample news article to see if there needs to be further preprocessing.\n",
    "\n",
    "__Print the news article at row 0__ to inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "33e53a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad sale boost time_warner profit quarterly profit media giant timewarner jump 76 $ 1.13bn Â£ 600 m month december $ 639 m year early firm big investor google benefit sale high speed internet connection high advert sale timewarner say fourth quarter sale rise 2 $ 11.1bn $ 10.9bn profit buoy gain offset profit dip warner_bros user aol time_warner say friday own 8 search engine google internet business aol mixed fortune lose 464,000 subscriber fourth quarter profit low precede quarter company say aol underlie profit exceptional item rise 8 strong internet advertising revenue hope increase subscriber offer online service free timewarner internet customer try sign aol exist customer high speed broadband timewarner restate 2000 2003 result follow probe the_us_securities_exchange_commission sec close conclude time_warner's fourth quarter profit slightly well analyst expectation film division see profit slump 27 $ 284 m help box office flop alexander catwoman sharp contrast year early final film lord rings trilogy boost result year timewarner post profit $ 3.36bn 27 2003 performance revenue grow 6.4 $ 42.09bn financial performance strong meet exceed year objective greatly enhance flexibility chairman chief executive richard_parsons say 2005 timewarner project operate earning growth 5 expect high revenue wide profit margin timewarner restate account effort resolve inquiry aol market regulator offer pay $ 300 m settle charge deal review sec company say unable estimate need set aside legal reserve previously set $ 500 m. intend adjust way account deal german music publisher bertelsmann purchase stake aol_europe report advertising revenue book sale stake aol_europe loss value stake\n"
     ]
    }
   ],
   "source": [
    "print(df.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec2a92",
   "metadata": {},
   "source": [
    "4d. The preprocessing that we've discussed in class are related to stop words and stemming.<br>\n",
    "\n",
    "__Create a Raw NBConvert cell to explain__:\n",
    "- Does it look like stop words have been removed? Give examples from the text.\n",
    "- Does it look like stemming was applied? Give examples from the text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4dddefc",
   "metadata": {},
   "source": [
    "Stop words have been removed. There is no \"a\", \"and\", \"in\", or other similar words.\n",
    "Stemming has been applied. All words are root words, such as \"enhance\" as opposed to \"enhanced\" \"enhances\" or \"enhancer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402320d",
   "metadata": {},
   "source": [
    "4e. __Convert the preprocessed data to numbers__ so it's ready for the ML model.<br>\n",
    "Then __print the shape of the X dataset__ that will be used with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "ed8169ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 65)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 148)\t1\n",
      "  (0, 365)\t1\n",
      "  (0, 372)\t1\n",
      "  (0, 377)\t2\n",
      "  (0, 379)\t1\n",
      "  (0, 500)\t2\n",
      "  (0, 519)\t1\n",
      "  (0, 551)\t1\n",
      "  (0, 629)\t1\n",
      "  (0, 691)\t1\n",
      "  (0, 729)\t1\n",
      "  (0, 771)\t1\n",
      "  (0, 860)\t1\n",
      "  (0, 883)\t1\n",
      "  (0, 971)\t1\n",
      "  (0, 1148)\t1\n",
      "  (0, 1593)\t2\n",
      "  (0, 1664)\t1\n",
      "  (0, 1726)\t1\n",
      "  (0, 1788)\t1\n",
      "  (0, 1792)\t2\n",
      "  :\t:\n",
      "  (0, 22802)\t1\n",
      "  (0, 22806)\t2\n",
      "  (0, 22815)\t1\n",
      "  (0, 22926)\t1\n",
      "  (0, 23132)\t1\n",
      "  (0, 23367)\t1\n",
      "  (0, 23401)\t1\n",
      "  (0, 23728)\t2\n",
      "  (0, 23922)\t3\n",
      "  (0, 24318)\t2\n",
      "  (0, 24396)\t2\n",
      "  (0, 26246)\t1\n",
      "  (0, 26513)\t3\n",
      "  (0, 26522)\t7\n",
      "  (0, 26942)\t1\n",
      "  (0, 26999)\t1\n",
      "  (0, 27177)\t1\n",
      "  (0, 27268)\t1\n",
      "  (0, 27666)\t1\n",
      "  (0, 27727)\t1\n",
      "  (0, 28185)\t1\n",
      "  (0, 28245)\t1\n",
      "  (0, 28325)\t1\n",
      "  (0, 28452)\t1\n",
      "  (0, 28841)\t4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2205, 28975)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X)\n",
    "X = vect.transform(X)\n",
    "print(X[0])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c582eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### as discussed in class, no need to show the vector. It doesn't mean much\n",
    "### to the reader            -1/4pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38ca54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd75688",
   "metadata": {},
   "source": [
    "5. Train and test the model\n",
    "\n",
    "5a. __Create X and y training and testing datasets__.<br>\n",
    "Then __print the shape of each dataset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "f573d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 28975) (552, 28975) (1653,) (552,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4815c02",
   "metadata": {},
   "source": [
    "5b. __Train and test the ML model__<br>\n",
    "and then __print the accuracy measurements__.\n",
    "\n",
    "_There are more than one accuracy measurement._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "560c2358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9710144927536232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[130,   0],\n",
       "       [  0,  85]], dtype=int64)"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "# accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "metrics.confusion_matrix(y_test, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc047b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### confusion matrix should be 5x5           -1/2pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307f708",
   "metadata": {},
   "source": [
    "5c. Create a Raw NBConvert cell to __discuss whether the accuracy measurements agree with each other__."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ded0711",
   "metadata": {},
   "source": [
    "The accuracy measurements do not agree with each other, because it should sit at 100%. Since in the confusion matrix the result was 133 predicted pos vs 133 pos, and 90 predicted neg and 90 neg, the accuracy should be 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the confusion matrix looks like 100% accurate probably because \n",
    "### the rest of it wasn't shown and the error could be in that part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df02be2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f28d74",
   "metadata": {},
   "source": [
    "6. Real life testing of the model you've trained.\n",
    "\n",
    "6a. __Print the lookup table__ you created in step 3c, which shows the corresponding values of the DocType and DocCat columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "82554d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Business', 1: 'Entertainment', 2: 'Politics', 3: 'Sport', 4: 'Tech'}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfb3ae",
   "metadata": {},
   "source": [
    "6b. One advantage of working in NLP is that it's easier to come up with testing data. \n",
    "\n",
    "1. Go to the [BBC News](https://www.bbc.com/news) website to find the different types of news categories.<br>\n",
    "2. __Choose 3 of the news categories__ in the BBC web page header that match the categories that the ML model have learned.<br>\n",
    "3. For each category, click on the category link to __find today's news articles in that category__.<br>\n",
    "> - Then click to open an article and __copy the first 4-5 paragraphs of the article__.<br>\n",
    "> - Create a Code cell and paste the paragraphs into a Python string.\n",
    "\n",
    "_You should end up with 3 Code cells, each has a Python string which is the 4-5 paragraphs of a news article._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "2d4553ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment = \"The film's production company Paramount Pictures said the injuries were non life-threatening and happened while shooting a planned stunt sequence. The crew members were all in stable condition and continue to receive treatment, the statement said. Earlier this week, the Sun reported there had been an explosion and six people went to hospital. It was terrifying - a huge ball of fire flew up and caught several crew members in its path. In years of filming I've never seen an accident so scary, a source told the newspaper. Everyone involved, from the lowliest runners to the star names, has been shaken up by this, they added. In a statement, a Paramount Pictures spokesperson said: The safety and full medical services teams on-site were able to act quickly so that those who were impacted immediately received necessary care. They said it has strict health and safety procedures in place on all our productions and would take all necessary precautions as we resume production. According to Variety, no cast members were injured but six people received treatment for burn injuries and four remain in hospital. Sir Ridley Scott, who directed the original 2000 historical drama film, is returning to direct the second instalment, which is scheduled to be released in November 2024. No title has yet been announced for the sequel, which stars Normal People actor Paul Mescal, Denzel Washington and Connie Nielson. The original film won five Oscars, including best actor for Russell Crowe, who played Roman general Maximus Decimus Meridius alongside Joaquin Phoenix as Emperor Commodus. The movie, set during the height of the Roman Empire, sees Maximus start out as a war hero before before being forced to become a gladiator. Gladiator made $457m (Â£355m) at the box office and revived the historical epic drama genre, which had been out of fashion for decades.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "61efbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "business = \"Meta has shown staff plans for a text-based social network designed to compete with Twitter, sources have told the BBC. It could allow users to follow accounts they already follow on Instagram, Meta's image-sharing app. And it could potentially allow them to bring over followers from decentralised platforms such as Mastodon. A Meta spokesperson confirmed to the BBC that the platform was in development. We're exploring a standalone decentralised social network for sharing text updates, they said. We believe there's an opportunity for a separate space where creators and public figures can share timely updates about their interests. Meta's chief product officer Chris Cox said coding was under way on the platform. The tech giant aims to release it soon, although no date was given. There is some speculation that it could be as early as the end of June. Screenshots have appeared online which were shown internally to employees, potentially giving an idea of what the app will look like. Sources within the company have told the BBC that these leaked screenshots are genuine. If they are, the layout of this new platform will be familiar to anyone who has spent time on Twitter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "be7a4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = \"A trial under way at Aberdeen Royal Infirmary is exploring whether artificial intelligence (AI) can assist radiologists in reviewing thousands of mammograms a year. The pilot helped spot early-stage breast cancer for June - a healthcare assistant and participant in the trial - and she is now set to undergo surgery as a result. Mammograms are low level X-rays used in breast cancer screenings to monitor and detect changes too small to see or feel. According to the NHS, they help save about 1,300 lives each year in the UK. And while the number of women who attended a routine breast screening, after an invitation, increased in Scotland in the three-year period to 2022, the number of radiologists to review results is shrinking. What is AI? AI - technology which sees computers perform specific tasks that would typically require human intelligence - is already widely used across a range of industries. While high-profile experts' fears that AI could lead to the extinction of humanity have recently been making headlines, the tech's more practical realities are already being shown in healthcare. Its potential to speed up the process of drug and disease discovery means many scientists and doctors see AI as a powerful tool to work with, rather than replace, practitioners.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a0367",
   "metadata": {},
   "source": [
    "6c. __Create a DataFrame from the 3 Python strings__.<br>\n",
    "Then __print the DataFrame__.\n",
    "\n",
    "_An example DataFrame is shown below, from news articles on 6/3. Your text will be different._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "dcdd8072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The film's production company Paramount Pictur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta has shown staff plans for a text-based so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A trial under way at Aberdeen Royal Infirmary ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The film's production company Paramount Pictur...\n",
       "1  Meta has shown staff plans for a text-based so...\n",
       "2  A trial under way at Aberdeen Royal Infirmary ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2 = pd.DataFrame(columns = [\"text\"],data = [[entertainment], [business], [tech]])\n",
    "display(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17156442",
   "metadata": {},
   "source": [
    "6d. __Test the ML model__ that you've trained with your new data in the DataFrame.\n",
    "\n",
    "This means:\n",
    "- preprocess the new data (your answer in step 4d will determine how you preprocess the new data).\n",
    "- convert the new data to numbers\n",
    "- test the model with the data\n",
    "- print the categories of news  that the model predicted. Use the lookup table to convert the numeric result from the model into the category string.\n",
    "Example:<br>`\n",
    "Article 1 : Sport\n",
    "Article 2 : Business\n",
    "Article 3 : Tech\n",
    "`\n",
    "\n",
    "_You'll need 4 Code cells, one for each step above_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "ecf61e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "#nltk.download('stopwords') \n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "tokenizer = RegexpTokenizer('[a-z]+')\n",
    "i = 0\n",
    "while i <= 2:\n",
    "    d2.text[i] = tokenizer.tokenize(d2.text[i].lower())\n",
    "    d2.text[i] = [d2.text[i] for d2.text[i] in d2.text[i] if d2.text[i] not in stop_words]\n",
    "    d2.text[i]=[stemmer.stem(w) for w in d2.text[i]]\n",
    "    d2.text[i] = ' '.join(d2.text[i])\n",
    "    i+=1\n",
    "\n",
    "#def preprocess(s) :\n",
    "#    w = tokenizer.tokenize(s.lower())         # separate into words and lowercase each word\n",
    "#    w = [word for word in w if word not in stop_words]    # remove stop words\n",
    "#    w = [stemmer.stem(word) for word in w]    # find stem of each word\n",
    "#    return ' '.join(w)         # join back into a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "a27d75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Temp\\ipykernel_20112\\2886702582.py:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  d2.text = vect.transform(d2.text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 28975)"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = CountVectorizer()\n",
    "v2.fit(d2.text)\n",
    "d2.text = vect.transform(d2.text)\n",
    "d2.text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### don't need a new vectorizer, use the same one as the training data\n",
    "### -1/2pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "c84d71ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0) (1, 0) (2, 28975) (1, 28975)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [2, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[846], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(A_train)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:749\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 749\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:583\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:778\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    774\u001b[0m     pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    775\u001b[0m         _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 778\u001b[0m         dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# array is a pandas series\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     pandas_requires_conversion \u001b[38;5;241m=\u001b[39m _pandas_dtype_needs_early_conversion(array\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "A = d2.drop(columns=['text'])\n",
    "b=d2.text\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.25)\n",
    "print(A_train.shape, A_test.shape, b_train.shape, b_test.shape)\n",
    "classifier = MultinomialNB()\n",
    "print(A_train)\n",
    "classifier.fit(X_train, b_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7898d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### why create training and testing set?  the 3 articles are all testing data    -1/2pt\n",
    "### don't create a new classifier. Use the one that you trained earlier   -1/2pt\n",
    "### there's no X_train and b_train to fit()    -1/2pt\n",
    "### there's no X_test   -1/2pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e497fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "# accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "metrics.confusion_matrix(y_test, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558aeae",
   "metadata": {},
   "source": [
    "6e. Create a Raw NBConvert cell to __discuss the result of your test__."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fef102a3",
   "metadata": {},
   "source": [
    "The articles I included weren't too specific to their category, and focused more on the details of the specific topic of the article, so there wasn't much similarity between the original trained ML and the final test, resulting in a low accuracy score and high confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8714db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### can't draw conclusion since the code above doesn't work so there was no prediction\n",
    "### and therefore no result to compare           -1pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
